{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aef7bad8-f164-456e-8e25-6cd6d902328d",
   "metadata": {},
   "source": [
    "##### Length of individual non-null rows for each lidar_data CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29e872d-d606-4fb0-b6ed-92ac49159a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"../data/training\"\n",
    "csv_files = [] # initialize empty list for storing CSV filenames\n",
    "\n",
    "for filename in os.listdir(data_folder): \n",
    "    if filename.endswith('.csv'):\n",
    "        csv_files.append(filename)\n",
    "        csv_filename = os.path.join(data_folder, filename)\n",
    "        df = pd.read_csv(csv_filename)\n",
    "        non_null_row_count = len(df.dropna())\n",
    "        print(f\"Non-null row count for '{filename}': {non_null_row_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f8b22a-22b3-4e3e-966c-968477a083dc",
   "metadata": {},
   "source": [
    "##### Approach of treating 'inf' values which were present with the use of a prior lidar sensor used in Webots (before switching to a new one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a32a370-02e9-4ea7-9797-ba5678cebc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_sensor_range = 0 # define an appropriate minimum sensor range\n",
    "X = X.replace([np.inf, -np.inf], min_sensor_range)\n",
    "\n",
    "# or max_sensor_range = 1000 # define an appropriate maximum sensor range\n",
    "X = X.replace([np.inf, -np.inf], max_sensor_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6090d7-1e84-425c-9427-e31471f7bdde",
   "metadata": {},
   "source": [
    "##### Loading compressed file (pickle - used to save space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af611ef-ec79-4d88-9346-7b2db92cdfb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os # allow us to interact with the operating system\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caca06e-176e-4bd6-8bd5-88ed5cc18315",
   "metadata": {},
   "source": [
    "Load a single pickle file and converting it into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339c80dc-8242-46c2-9f10-55845233cd28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle_filename = \"../data/traininglidar_data4.pkl\"\n",
    "with open(pickle_filename, 'rb') as pickle_file:\n",
    "    unpickled_data = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd449b83-54d6-404e-83f0-32db3f64dcf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a DataFrame from unpickled data\n",
    "unpickled_data = pd.DataFrame(unpickled_data)\n",
    "\n",
    "# unpickled_df.iloc[0:11, 14:]\n",
    "display(unpickled_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea064ddb-5f52-41d2-a97a-79f284af9157",
   "metadata": {},
   "source": [
    "When the training data was saved as individual .pkl files (alongside the CSVs), load all of them located inside the 'data_folder' and combine them into a single DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fe9c63-f997-4981-a3f1-d5fa97b22f2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_folder = \"../data/training\"\n",
    "unpickled_dfs = []\n",
    "\n",
    "for filename in os.listdir(data_folder):\n",
    "    if filename.endswith(\".pkl\"):\n",
    "        pickle_filename = os.path.join(data_folder, filename)\n",
    "        with open(pickle_filename, 'rb') as pkl_file:\n",
    "            unpickled_data = pickle.load(pkl_file)\n",
    "            df = pd.DataFrame(unpickled_data)\n",
    "            unpickled_dfs.append(df)\n",
    "            print(f\"Length of {filename}: {len(df)}\")\n",
    "\n",
    "# concatenate all dataframes into one\n",
    "unpkled_combined_df = pd.concat(unpickled_dfs, ignore_index=True)\n",
    "\n",
    "# print the length of the final combined DataFrame\n",
    "print(f\"Length of the final combined DataFrame: {len(unpkled_combined_df)}\")\n",
    "\n",
    "# display the final combined DataFrame\n",
    "unpkled_combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f20040-60b5-40f8-b1b9-0b4c60c6b844",
   "metadata": {},
   "source": [
    "##### Loading compressed file (joblib with gzip=3 compression - used to save space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea148d0-8f8e-417c-8637-4fc40b0efd13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib_filename = \"..data/training/lidar_data8.joblib1.gz\"\n",
    "with open(joblib_filename, 'rb') as joblib_file:\n",
    "    loaded_data = joblib.load(joblib_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e61521-ca55-402e-b293-c629eb153645",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a DataFrame from unpickled data\n",
    "loaded_df = pd.DataFrame(loaded_data)\n",
    "\n",
    "# loaded_df.iloc[0:11, 14:]\n",
    "display(loaded_df.head())"
   ]
  },
  {
   "attachments": {
    "5d9506ad-f4ed-4788-bd48-df69edc723d2.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "effc78ff-5349-4ab7-a7d4-05bb05563cd5",
   "metadata": {},
   "source": [
    "During training, each file was saved as a CSV, pickle, and joblib file. The goal was to ideally to only store compressed training files to save space and load them to create a single DataFrame which would be used to train models on for testing. However, Webots had an issue with the compression of these pickle and joblib files due to errors that resulting from the file sizes being too large resulting in longer than normal compression and loading times, inevitably causing it to timeout. \n",
    "![webots_compression_errors.png](attachment:5d9506ad-f4ed-4788-bd48-df69edc723d2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86625411-ce3f-4cf4-aebd-63ab5a155189",
   "metadata": {},
   "source": [
    "Below is an example of compression for a trained random forest classifier model (using \"default\" parameters) with a compression gzip=5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f9dfad-f422-4c70-9a34-a25b426092c3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "\n",
    "# remove rows that contain NaN values (for keyboard input, y)\n",
    "unpkled_combined_df = unpkled_combined_df.dropna()\n",
    "\n",
    "# extract features (sensor readings) and labels (control inputs)\n",
    "X = unpkled_combined_df.iloc[:, 1:]  # sensor readings\n",
    "y = unpkled_combined_df.iloc[:, 0]   # keyboard input\n",
    "\n",
    "max_sensor_range = 1000  # define an appropriate maximum sensor range\n",
    "X = X.replace([np.inf, -np.inf], max_sensor_range)\n",
    "\n",
    "# display(X.iloc[0:11, 14:])\n",
    "\n",
    "# create and train the model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# train the model on the entire dataset\n",
    "model.fit(X, y)\n",
    "\n",
    "# save the trained model to a file using joblib\n",
    "joblib.dump(model, 'trained_model.joblib', compress=('gzip', 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc32de05-f7b6-4589-a001-78996c04ba2e",
   "metadata": {},
   "source": [
    "##### Used to compare the differences between the same CSV files but had different sizes (i.e. different number of kilobytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563183b6-79f3-49ef-815d-485263a3ab11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "path1 = \"../data/model/lidar_data.csv\"\n",
    "path2 = \"../data/model/lidar_data_combined.csv\"\n",
    "\n",
    "with open(path1) as f1, open(path2) as f2:\n",
    "    reader1 = csv.reader(f1)\n",
    "    reader2 = csv.reader(f2)\n",
    "\n",
    "    for i, row1 in enumerate(reader1):\n",
    "        try:\n",
    "            row2 = next(reader2)\n",
    "        except StopIteration:\n",
    "            print(f\"Row {i+1}, f1 has this extra row compared to f2\")\n",
    "            continue\n",
    "\n",
    "        if row1 == row2:\n",
    "            continue\n",
    "\n",
    "        if len(row1) != len(row2):\n",
    "            print(f\"Row {i+1} of f1 has {len(row1)} cols, f2 has {len(row2)} cols\")\n",
    "            continue\n",
    "\n",
    "        for j, cell1 in enumerate(row1):\n",
    "            cell2 = row2[j]\n",
    "            if cell1 != cell2:\n",
    "                print(f'Row {i+1}, Col {j+1} of f1 is \"{cell1}\", f2 is \"{cell2}\"')\n",
    "\n",
    "    for row2 in reader2:\n",
    "        i += 1\n",
    "        print(f\"Row {i+1}, f2 has this extra row compared to f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621259a6-c725-432f-a910-83b05b4b0c4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def compare_csv_files(file1, file2):\n",
    "    # read the CSV files into pandas DataFrames\n",
    "    df1 = pd.read_csv(file1)\n",
    "    df2 = pd.read_csv(file2)\n",
    "\n",
    "    # check if the DataFrames are equal\n",
    "    are_equal = df1.equals(df2)\n",
    "\n",
    "    if are_equal:\n",
    "        print(\"The data in the two CSV files is identical.\")\n",
    "    else:\n",
    "        print(\"Differences found between the two CSV files:\")\n",
    "        # find and display the specific differences\n",
    "        diff = df1.compare(df2)\n",
    "        print(diff)\n",
    "\n",
    "# paths to the two CSV files\n",
    "path1 = \"../data/model/lidar_data.csv\"\n",
    "path2 = \"../data/model/lidar_data_combinedog.csv\"\n",
    "\n",
    "# compare the CSV files\n",
    "compare_csv_files(path1, path2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5686da56-e3cd-4b3f-9359-52334fb45e58",
   "metadata": {},
   "source": [
    "##### Generate video of lidar range (distance measurements) over simulated time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1f01bd-6cf8-49cf-9b6c-155d9ee823cf",
   "metadata": {},
   "source": [
    "Test environment 2 - Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba19ff7-5212-4fc5-906f-7350324fee8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import seaborn as sb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load the CSV file for the Random Forest Classifier\n",
    "csv_data = pd.read_csv(\"../data/testing/env1/lidar_data_rfc_model1_env1.csv\")\n",
    "\n",
    "# calculate the maximum and minimum range values\n",
    "range_max = 1.1 * csv_data.iloc[:, 1:].max().max()\n",
    "range_min = 0\n",
    "\n",
    "vid_obj = cv2.VideoWriter('rfc_lidar_range.avi', cv2.VideoWriter_fourcc(*'DIVX'), 32, (640,480))\n",
    "laser_idx = np.arange(128)\n",
    "for k in range(0, len(csv_data), 10):\n",
    "    key = csv_data['Predicted Action'].iloc[k]\n",
    "    ranges = csv_data.iloc[k,1:]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax = sb.lineplot(x=laser_idx,y=ranges)\n",
    "    ax.set_title(key)\n",
    "    ax.set_xlabel('Laser Index')\n",
    "    ax.set_ylabel('Laser Range')\n",
    "    ax.set_ylim(range_min,range_max)\n",
    "\n",
    "    fig.canvas.draw()\n",
    "    frame = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "    frame = frame.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "\n",
    "    vid_obj.write(frame)\n",
    "    plt.close()\n",
    "\n",
    "vid_obj.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e381b4-3072-4b21-80aa-5fd7ceee9bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.1*csv_data.iloc[:,1:].max().max()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lhl_env",
   "language": "python",
   "name": "lhl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
